<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>RWKV Language Model</title>
    <link
      href="https://fonts.font.im/css?family=Lora:400,700|Open+Sans:400,300,800,700"
      rel="stylesheet"
      type="text/css"
    />
    <meta name="description" content="The RWKV Language Model" />
    <link rel="stylesheet" href="css/main.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
    <link
      href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css"
      rel="stylesheet"
    />
    <style>
      .resume-item {
        display: flex;
      }

      .content-section .resume-item-copy {
        margin-block-start: 0;
        margin-inline-start: 1rem;
      }

      .wrapper {
        padding: 0 12px;
      }
    </style>
  </head>

  <body class="theme-default">
    <div class="wrapper" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="telephone" content="" />
      <meta itemprop="address" content="rwkv.com" />
      <header class="page-header">
        <img
          src="images/avatar.png"
          alt="RWKV"
          class="avatar"
          style="width: 100%"
          itemprop="image"
        />
        <div class="title-bar" style="margin-top: 1em">
          <h2 class="header-title" itemprop="jobTitle">RWKV Language Model</h2>
          <ul class="icon-links">
            <li class="icon-link-item">
              <a
                href="https://github.com/BlinkDL/RWKV-LM"
                class="icon-link"
                itemprop="sameAs"
                ><svg
                  class="icon"
                  xmlns="http://www.w3.org/2000/svg"
                  xmlns:xlink="http://www.w3.org/1999/xlink"
                  x="0px"
                  y="0px"
                  viewBox="0 0 28 28"
                  enable-background="new 0 0 28 28"
                  xml:space="preserve"
                  width="28"
                >
                  <path
                    id="GitHub"
                    fill-rule="evenodd"
                    clip-rule="evenodd"
                    fill="#D1CECC"
                    d="M14.01,0C6.27,0-0.01,6.28-0.01,14.02
  c0,6.19,4.02,11.45,9.59,13.3c0.7,0.13,0.96-0.3,0.96-0.68c0-0.33-0.01-1.21-0.02-2.38c-3.9,0.85-4.72-1.88-4.72-1.88
  c-0.64-1.62-1.56-2.05-1.56-2.05c-1.27-0.87,0.1-0.85,0.1-0.85c1.41,0.1,2.15,1.44,2.15,1.44c1.25,2.14,3.28,1.52,4.08,1.16
  c0.13-0.91,0.49-1.52,0.89-1.87c-3.11-0.35-6.38-1.56-6.38-6.93c0-1.53,0.55-2.78,1.44-3.76C6.37,9.17,5.89,7.74,6.65,5.81
  c0,0,1.18-0.38,3.85,1.44c1.12-0.31,2.32-0.47,3.51-0.47c1.19,0.01,2.39,0.16,3.51,0.47c2.68-1.81,3.85-1.44,3.85-1.44
  c0.76,1.93,0.28,3.35,0.14,3.71c0.9,0.98,1.44,2.23,1.44,3.76c0,5.38-3.28,6.57-6.4,6.92c0.5,0.43,0.95,1.29,0.95,2.6
  c0,1.87-0.02,3.39-0.02,3.84c0,0.37,0.25,0.81,0.96,0.67c5.56-1.86,9.58-7.11,9.58-13.3C28.03,6.28,21.75,0,14.01,0z"
                  />
                </svg>
              </a>
            </li>
            <li class="icon-link-item">
              <a
                href="https://twitter.com/BlinkDL_AI"
                class="icon-link"
                itemprop="sameAs"
                ><svg
                  class="icon"
                  xmlns="http://www.w3.org/2000/svg"
                  xmlns:xlink="http://www.w3.org/1999/xlink"
                  x="0px"
                  y="0px"
                  viewBox="0 0 28 28"
                  enable-background="new 0 0 28 28"
                  xml:space="preserve"
                  width="28"
                >
                  <path
                    id="Twitter"
                    fill="#D1CECC"
                    d="M14,0C6.27,0,0,6.27,0,14s6.27,14,14,14s14-6.27,14-14S21.73,0,14,0z M20.69,10.57
  c0.01,0.15,0.01,0.3,0.01,0.45c0,4.56-3.47,9.82-9.82,9.82c-1.95,0-3.76-0.57-5.29-1.55c0.27,0.03,0.54,0.05,0.82,0.05
  c1.62,0,3.11-0.55,4.29-1.48c-1.51-0.03-2.79-1.03-3.23-2.4c0.21,0.04,0.43,0.06,0.65,0.06c0.31,0,0.62-0.04,0.91-0.12
  c-1.58-0.32-2.77-1.71-2.77-3.39c0-0.01,0-0.03,0-0.04c0.47,0.26,1,0.41,1.56,0.43c-0.93-0.62-1.54-1.68-1.54-2.87
  c0-0.63,0.17-1.23,0.47-1.74c1.7,2.09,4.25,3.46,7.12,3.61c-0.06-0.25-0.09-0.52-0.09-0.79c0-1.91,1.55-3.45,3.45-3.45
  c0.99,0,1.89,0.42,2.52,1.09c0.79-0.15,1.53-0.44,2.19-0.84c-0.26,0.81-0.81,1.48-1.52,1.91c0.7-0.08,1.36-0.27,1.98-0.54
  C21.95,9.47,21.37,10.08,20.69,10.57z"
                  />
                </svg>
              </a>
            </li>
          </ul>
        </div>
        <div class="executive-summary" itemprop="description">
          <p>
            RWKV (pronounced RwaKuv) is an RNN with GPT-level LLM performance,
            and can also be directly trained like a GPT transformer
            (parallelizable). We are at <a href="#rwkv6_img"><b>RWKV v6</b></a
            >.
          </p>
          <p>
            So it's combining the best of RNN and transformer - great
            performance, fast inference, fast training, saves VRAM, "infinite"
            ctxlen, and free text embedding. Moreover it's 100% attention-free,
            and a
            <a href="https://lfaidata.foundation/projects/rwkv/">LFAI project</a
            >.
          </p>
        </div>
        <div style="display: flex; margin-top: -1rem; gap: 0.3rem">
          <a
            href="https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1"
            class="contact-button"
            >v6 3B Demo</a
          >
          <a
            href="https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2"
            class="contact-button"
            >v6 7B Demo</a
          >
          <a href="https://discord.gg/bDSBUMeFpc" class="contact-button"
            >Discord <span style="font-size: 70%">(7k members)</span></a
          >
        </div>
        <a href="https://arxiv.org/abs/2404.05892"
          ><img
            src="images/RWKV-paper.png"
            style="width: 100%"
            alt="RWKV paper"
            itemprop="image"
        /></a>
        <div style="margin-bottom: 0.5rem"></div>
      </header>

      <section class="content-section">
        <header class="section-header">
          <h2>RWKV-Papers</h2>
        </header>
          <!-- 分类标签部分 -->
          <div
            class="flex flex-wrap justify-center gap-2 py-4 bg-gray-100 rounded-lg mb-6 border border-gray-300 shadow-sm"
          >
            <button
              onclick="filterProjects('ALL', this)"
              class="filter-btn px-4 py-2 text-sm font-semibold bg-blue-600 text-white rounded-md hover:bg-blue-700 transition duration-200 ease-in-out shadow-sm"
            >
              ALL
            </button>
            <button
            onclick="filterProjects('Language Model', this)"
            class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
          >
          Language Model
          </button>
            <button
              onclick="filterProjects('Sequence Model', this)"
              class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
            >
              Sequence Model
            </button>
            <button
              onclick="filterProjects('Visual', this)"
              class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
            >
              Visual
            </button>
            <button
            onclick="filterProjects('Audio', this)"
            class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
          >
            Audio
          </button>
            <button
              onclick="filterProjects('Robotics', this)"
              class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
            >
              Robotics
            </button>
            <button
              onclick="filterProjects('Other', this)"
              class="filter-btn px-4 py-2 text-sm font-semibold bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition duration-200 ease-in-out shadow-sm"
            >
              Other
            </button>
          
          </div>

          <!-- 项目展示区域 -->
          <main id="projectGrid" class="grid grid-cols-2 gap-6">
            <!-- 动态插入项目卡片 -->
          </main>

          <!-- 分页显示 -->
          <div class="flex justify-between items-center mt-6">
            <span id="totalCount" class="text-gray-600 text-sm"
              >共 0 篇相关文章</span
            >
            <!-- 分页按钮 -->
            <div class="flex items-center space-x-1" id="pagination">
              <!-- 动态插入分页按钮 -->
            </div>
          </div>
          <header class="section-header">
            <h2>RWKV-Projects</h2>
          </header>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://github.com/BlinkDL/RWKV-LM" itemprop="url"
              >RWKV-LM</a
            >
          </h3>
          <p class="resume-item-copy">Training RWKV</p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://github.com/josStorer/RWKV-Runner" itemprop="url"
              >RWKV-Runner</a
            >
          </h3>
          <p class="resume-item-copy">
            RWKV GUI with one-click install and API
          </p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://pypi.org/project/rwkv/" itemprop="url"
              >RWKV pip package</a
            >
          </h3>
          <p class="resume-item-copy">Official RWKV pip package</p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://github.com/JL-er/RWKV-PEFT" itemprop="url"
              >RWKV-PEFT</a
            >
          </h3>
          <p class="resume-item-copy">
            Finetuning RWKV (9GB VRAM can finetune 7B)
          </p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a
              href="https://github.com/cgisky1980/ai00_rwkv_server"
              itemprop="url"
              >RWKV-server</a
            >
          </h3>
          <p class="resume-item-copy">
            Fast WebGPU inference (NVIDIA/AMD/Intel), nf4/int8/fp16
          </p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://github.com/saharNooby/rwkv.cpp" itemprop="url"
              >RWKV.cpp</a
            >
          </h3>
          <p class="resume-item-copy">
            Fast CPU/cuBLAS/CLBlast inference, int4/int8/fp16/fp32
          </p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a
              href="https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories"
              itemprop="url"
              >More... (300+ RWKV projects)</a
            >
          </h3>
          <p class="resume-item-copy"></p>
        </div>
      </section>

      <section class="content-section">
        <header class="section-header">
          <h2>Misc</h2>
        </header>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://huggingface.co/BlinkDL" itemprop="url"
              >RWKV raw weights</a
            >
          </h3>
          <p class="resume-item-copy">All latest RWKV weights</p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://huggingface.co/RWKV" itemprop="url"
              >RWKV weights</a
            >
          </h3>
          <p class="resume-item-copy">HuggingFace-compatible RWKV weights</p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a
              href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/rwkv_v6_demo.py"
              itemprop="url"
              >RWKV v6 explained</a
            >
          </h3>
          <p class="resume-item-copy"></p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a
              href="https://scholar.google.com/scholar?scisbd=2&q=rwkv&as_sdt=0,5"
              itemprop="url"
              >RWKV-related papers</a
            >
          </h3>
          <p class="resume-item-copy"></p>
        </div>
        <div
          class="resume-item"
          itemscope
          itemtype="http://schema.org/CreativeWork"
        >
          <h3 class="resume-item-title" itemprop="name">
            <a href="https://wiki.rwkv.com/" itemprop="url">RWKV wiki</a>
          </h3>
          <p class="resume-item-copy">Community wiki (with guide and FAQ)</p>
        </div>
        <div id="rwkv6_img">
          <img src="images/RWKV-6-MQAR.png" style="width: 100%" />
          <p style="font-weight: bold; font-size: 150%">
            RWKV v6 illustrated (<a href="https://huggingface.co/BlinkDL/temp"
              >download preview checkpts</a
            >):
          </p>
          <img src="images/rwkv-x060.jpg" style="width: 100%" />
        </div>
      </section>

      <footer class="page-footer">
        <p class="footer-line">RWKV.com</p>
      </footer>
    </div>

    <script>
      // 示例数据
      const allProjects = [
      {
          title: "Video RWKV:Video Action Recognition Based RWKV",
          description: "The paper proposes LSTM CrossRWKV (LCR) for video understanding. It uses a novel CrossRWKV gate to handle video challenges. LCR stores long-term memory and reduces redundant information. Experiments on datasets show its effectiveness, setting a new benchmark in video understanding with RWKV.",
          date: "2024-11-8",
          tags: "Visual",
          img: "images/papers-images/video-rwkv.png",
          link: "https://arxiv.org/abs/2411.05636"
        },
        {
          title: "From Explicit Rules to Implicit Reasoning in an Interpretable Violence Monitoring System",
          description: "The paper proposes RuleVM for weakly supervised violence monitoring. It uses a dual-branch structure with different designs for images and text. The implicit branch uses visual features for coarse-grained classification, and the explicit branch uses language-image alignment with YOLO-World and data mining. RWKV is used in the lightweight time-series module.",
          date: "2024-10-29",
          tags: "Other",
          img: "images/papers-images/pipeline-of-RuleVM-system.png",
          link: "https://arxiv.org/abs/2410.21991"
        },
        {
          title: "Modern Sequence Models in Context of Multi-Agent Reinforcement Learning",
          description: "The paper focuses on MARL. It proposes MAM and MARWKV architectures inspired by MAT. Experiments show they perform comparably to MAT. MARWKV offers better inference computational efficiency, especially with more agents. RWKV is used in MARWKV for sequence modeling.",
          date: "2024-10-28",
          tags: "Sequence Model",
          img: "images/papers-images/marwkv-architecture.png",
          link: "https://epub.jku.at/obvulihs/content/titleinfo/10580112"
        },
        {
          title: "MATCC: A Novel Approach for Robust Stock Price Prediction Incorporating Market Trends and Cross-time Correlations",
          description: "Stock price prediction is challenging. Existing work has limitations. This paper proposes MATCC, a novel framework. It extracts market trends, decomposes stock data, and mines cross-time correlation. Experiments show MATCC outperforms previous works. It uses RWKV to model inter-temporal correlations",
          date: "2024-10-21",
          tags: "Other",
          img: "images/papers-images/matcc-cumulative-return-comparison.png",
          link: "https://dl.acm.org/doi/abs/10.1145/3627673.3679715"
        },
        {
          title: "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models",
          description: "The paper presents VisualRWKV-HD and VisualRWKV-UHD for high-resolution visual inputs in visual language models. It details techniques like lossless downsampling and image segmentation. Experiments on benchmarks show their effectiveness, with RWKV models achieving better performance in handling high-resolution tasks.",
          date: "2024-10-15",
          tags: "Visual",
          img: "images/papers-images/VisualRWKV-HD-UHD-Architecture_Design.png",
          link: "https://arxiv.org/abs/2410.11665"
        },
        {
          title: "AttnInput: Revolutionizing Pinyin Input with Context-Aware RWKV Language Models",
          description: "The paper presents AttnInput, a novel approach leveraging RWKV for Pinyin IME. It integrates Pinyin into RWKV's internal state, addressing semantic discontinuity. Using a pre-training strategy, it reduces costs. Experimental results show it achieves state-of-the-art performance on abbreviated Pinyin input.",
          date: "2024-10-13",
          tags: "Other",
          img: "images/papers-images/rwkv-attninput-architecture.png",
          link: "https://openreview.net/forum?id=9OxTqscUwi"
        },
        {
          title: "OccRWKV: Rethinking Efficient 3D Semantic Occupancy Prediction with Linear Complexity",
          description: "The paper presents OccRWKV, an efficient 3D semantic occupancy network inspired by RWKV. It separates predictions into branches with Sem-RWKV and GeoRWKV blocks. By projecting features to BEV space and using BEV-RWKV block, it achieves real-time inference. It outperforms state-of-the-art methods on SemanticKITTI dataset",
          date: "2024-09-26",
          tags: "Robotics",
          img: "images/papers-images/occrwkv-architecture.jpg",
          link: "https://www.arxiv.org/abs/2409.19987"
        },
        {
          title: "Multi-scale RWKV with 2-dimensional temporal convolutional network for short-term photovoltaic power forecasting",
          description: "The paper proposes MSRWKV-2DTCN for short-term PV power forecasting. It uses FFT to identify periodicity, combines RWKV with a multi-scale 2D TCN, and conducts experiments on real datasets. The model shows high accuracy and strong generalization capabilities.",
          date: "2024-09-06",
          tags: "Sequence Model",
          img: "images/papers-images/MSRWKV-2DTCN-architecture.png",
          link: "https://www.sciencedirect.com/science/article/abs/pii/S0360544224028433"
        },
        {
          title: "Experimentation in Content Moderation using RWKV",
          description: "The paper investigates RWKV's efficacy in content moderation. It creates a novel dataset for distillation, generates responses using LLMs, and fine-tunes RWKV. The study shows RWKV can improve content moderation accuracy and efficiency, and paves the way for more efficient models.",
          date: "2024-09-05",
          tags: "Other",
          img: "images/papers-images/mod-rwkv-architecture.png",
          link: "https://arxiv.org/abs/2409.03939"
        },
        {
          title: "Temporal and Interactive Modeling for Efficient Human-Human Motion Generation",
          description: "The paper presents TIM for efficient human-human motion generation. It proposes Causal Interactive Injection, Role-Evolving Mixing, and Localized Pattern Amplification. Experiments on InterHuman show TIM's superiority, achieving state-of-the-art results with only 32% of InterGen's trainable parameters, using RWKV",
          date: "2024-08-30",
          tags: "Visual",
          img: "images/papers-images/rwkv-tim-architecture.png",
          link: "https://arxiv.org/abs/2408.17135"
        }, {
          title: "OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameter",
          description: "The paper explores a small sports-domain language model. It creates the OnlySports collection (dataset, benchmark, LM). Using 600 billion tokens data, it optimizes RWKV-v6 for sports tasks, training a 196M param model. OnlySportsLM outperforms prior models and rivals larger ones in the sports domain.",
          date: "2024-08-30",
          tags: "Language Model",
          img: "images/papers-images/onlysportslm-table.png",
          link: "https://arxiv.org/abs/2409.00286"
        },
        {
          title: "Why Perturbing Symbolic Music is Necessary: Fitting the Distribution of Never-used Notes through a Joint Probabilistic Diffusion Model",
          description: "The paper propose the Music-Diff architecture, which uses a joint probabilistic diffusion model. It improves note distribution fitting and sample diversity compared to language models like RWKV-music, enhancing rhythmic and structural coherence in generated music.",
          date: "2024-08-04",
          tags: "Audio",
          img: "images/papers-images/symb-rwkv-for-music-diff.png",
          link: "https://arxiv.org/abs/2408.01950"
        },
        {
          title: "Optimizing Robotic Manipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for Lifelong Learning",
          description: "The paper explores RWKV's integration with decision transformer and experience replay in robotic manipulation. It proposes the Decision-RWKV model, tests it on D4RL and D’Claw platforms, and shows its effectiveness in single-task and lifelong learning, with code open-sourced.",
          date: "2024-07-23",
          tags: "Robotics",
          img: "images/papers-images/Decision-RWKV-block-overview.png",
          link: "https://arxiv.org/abs/2407.16306"
        },
        {
          title: "BSBP-RWKV: Background Suppression with Boundary Preservation for Efficient Medical Image Segmentation",
          description: "The paper proposes BSBP-RWKV for accurate and efficient medical image segmentation. It combines the advantages of PMD and RWKV, devises DWT-PMD RWKV Block and Multi-Step Runge-Kutta convolutional Block, and proposes a novel loss function. Experiments show its superior accuracy and efficiency.",
          date: "2024-07-21",
          tags: "Visual",
          img: "images/papers-images/BSBP-RWKV-architecture.png",
          link: "https://openreview.net/pdf?id=ULD5RCk0oo"
        },
        {
          title: "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression",
          description: "The paper presents GoldFinch, a hybrid Linear Attention/Transformer model. It uses a new technique to generate a highly compressed KV-Cache. GoldFinch stacks GOLD transformer on an enhanced RWKV-6 (Finch) architecture. It shows improved performance with reduced cache size compared to Finch and Llama.",
          date: "2024-07-16",
          tags: "Sequence Model",
          img: "images/papers-images/GoldFinch-architecture.png",
          link: "https://arxiv.org/abs/2407.12077"
        },
        {
          title: "Restore-RWKV: Efficient and Effective Medical Image Restoration with RWKV",
          description: "The paper of this paper proposes Restore-RWKV, the first RWKV-based model for medical image restoration. It modifies RWKV's attention and token shift layers to handle 2D images, capturing global and local dependencies. Experiments show its superiority in various tasks, serving as an efficient and effective backbone.",
          date: "2024-07-14",
          tags: "Visual",
          img: "images/papers-images/restore-rwkv-architecture.png",
          link: "https://arxiv.org/abs/2407.11087"
        },
        {
          title: "Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model",
          description: "RThe paper focuses on designing an efficient segment-anything model. It proposes RWKV-SAM with a mixed backbone of convolution and RWKV operation. This model achieves high accuracy and efficiency, outperforming others in benchmarks. It also trains on a combined high-quality dataset for better segmentation.",
          date: "2024-06-27",
          tags: "Visual",
          img: "images/papers-images/rwkv-sam-architecture.png",
          link: "https://arxiv.org/abs/2406.19369"
        },
        {
          title: "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models",
          description: "The paper presents VisualRWKV, the first application of the linear RNN model RWKV in multimodal learning. It proposes novel mechanisms like data-dependent recurrence. Experiments show it performs competitively compared to Transformer models, with efficient computation and memory usage.",
          date: "2024-06-19",
          tags: "Visual",
          img: "images/papers-images/visual-rwkv-architecture.png",
          link: "https://arxiv.org/abs/2406.13362"
        },
        {
          title: "RWKV-CLIP: A Robust Vision-Language Representation Learner",
          description: "The paper explores CLIP from data and model architecture perspectives. It proposes a diverse description generation framework and RWKV-CLIP, the first RWKV-driven vision-language model. Experiments show RWKV-CLIP's robustness and effectiveness, achieving state-of-the-art performance in multiple downstream tasks.",
          date: "2024-06-11",
          tags: "Visual",
          img: "images/papers-images/rwkv-clip-architecture.png",
          link: "https://arxiv.org/abs/2406.06973"
        },
        {
          title: "PointRWKV: Efficient RWKV-Like Model for Hierarchical Point Cloud Learning",
          description: "The paper proposes PointRWKV, a new model with linear complexity adapted from RWKV in NLP for 3D point cloud learning. It uses modified multi-headed matrix-valued states and a dynamic attention recurrence mechanism to explore global processing capabilities and a parallel branch to encode local geometric features, outperforming other models and saving FLOPs.",
          date: "2024-05-24",
          tags: "Visual",
          img: "images/papers-images/point-rwkv--architecture.png",
          link: "https://arxiv.org/abs/2405.15214"
        },
        {
          title: "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
          description: "The paper presents Eagle (RWKV-5) and Finch (RWKV-6), improving RWKV-4. Their architectural enhancements include multiheaded matrix-valued states and dynamic recurrence. New multilingual corpus and tokenizer are introduced. Trained models show competitive performance, and all are publicly released.",
          date: "2024-04-08",
          tags: "Language Model",
          img: "images/papers-images/rwkv-5-6-architecture.png",
          link: "https://arxiv.org/abs/2404.05892"
        },
        {
          title: "Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models",
          description: "The paper presents Diffusion-RWKV, an architecture adapting RWKV for diffusion models in image generation. It handles long-range hidden states linearly, showing comparable performance to Transformers but with lower complexity, thus being a promising alternative in this field.",
          date: "2024-04-06",
          tags: "Visual",
          img: "images/papers-images/Diffusion-RWKV-architecture.png",
          link: "https://arxiv.org/abs/2404.04478"
        },
        {
          title: "Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention",
          description: "Deep learning in spacecraft hyperspectral image compression was challenging. This paper designs LineRWKV, a predictive neural network. It uses a novel hybrid operation, combines Transformers & RNNs. LineRWKV outperforms CCSDS-123.0-B-2 in compression and shows good throughput on a 7W system.",
          date: "2024-03-26",
          tags: "Visual",
          img: "images/papers-images/LineRWKV-architecture.png",
          link: "https://arxiv.org/abs/2403.17677"
        },
        {
          title: "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures",
          description: "The paper presents Vision-RWKV (VRWKV), an adaptation of the RWKV model for vision tasks. It offers efficient handling of sparse inputs and strong global processing, with reduced spatial aggregation complexity. VRWKV outperforms ViT in image classification and shows advantages in dense prediction tasks, being a promising alternative for visual perception.",
          date: "2024-03-07",
          tags: "Visual",
          img: "images/papers-images/Vision-RWKV-architecture.png",
          link: "https://arxiv.org/abs/2403.02308"
        },
        {
          title: "TLS-RWKV: Real-Time Online Action Detection with Temporal Label Smoothing",
          description: "The paper proposes TLS-RWKV for online action detection. It utilizes the RWKV model with temporal label smoothing. Experiments on THUMOS’14 and TVSeries datasets show state-of-the-art performance and high efficiency, making it suitable for real-time applications and resource-constrained devices.",
          date: "2024-2-19",
          tags: "Visual",
          img: "images/papers-images/TLS-RWKV-architecture.png",
          link: "https://link.springer.com/article/10.1007/s11063-024-11540-0"
        },
        {
          title: "SDiT: Spiking Diffusion Model with Transformer",
          description: "The paper proposes Spiking Diffusion Transformer (SDiT), a novel SNN diffusion model. It uses RWKV for efficient self-attention. SDiT aims to provide a baseline for SNN generative models and shows competitiveness on multiple datasets, generating high-quality images with lower cost and shorter sampling time.",
          date: "2024-02-18",
          tags: "Visual",
          img: "images/papers-images/sdit-architecture.png",
          link: "https://arxiv.org/abs/2402.11588"
        },
        {
          title: "RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks",
          description: "Traditional RNNs have declined in time series tasks. This paper presents RWKV-TS, an efficient RNN-based model. It has O(L) complexity, captures long-term info well, and is computationally efficient. RWKV-TS shows competitive performance with reduced latency and memory use in various tasks.",
          date: "2024-01-17",
          tags: "Sequence Model",
          img: "images/papers-images/rwkv-ts-architecture.png",
          link: "https://arxiv.org/abs/2401.09093"
        },
        {
          title: "Advancing VAD Systems Based on Multi-Task Learning with Improved Model Structures",
          description: "The paper proposes semantic VAD systems based on multi-task learning with improved models (RWKV for real-time, SAN-M for offline) to address issues in traditional binary VAD. Evaluations show significant improvements in CER, DCF, and NRR metrics compared to DFSMN-based systems.",
          date: "2023-12-19",
          tags: "Audio",
          img: "images/papers-images/rwkv-vad--architecture.png",
          link: "https://arxiv.org/abs/2312.14860"
        },
        {
          title: "RWKV-based Encoder-Decoder Model for Code Completion",
          description: "The paper presents an RWKV-based encoder-decoder model for code completion. It aims to address challenges in this area. The model shows good performance and has potential for improving code generation efficiency, but more research is needed for wider application and optimization.",
          date: "2023-11-17",
          tags: "Language Model",
          img: "images/papers-images/RWKV-Code-Completion.png",
          link: "https://ieeexplore.ieee.org/abstract/document/10442108"
        },
        {
          title: "RWKV: A Linear Attention Mechanism for Temperature and Humidity Compensation for Gas Sensors",
          description: "The paper presents a novel methodology for a PANI-CeO2 ammonia gas sensor to address temperature and humidity compensation. It uses the RWKV network with a Linear attention mechanism. The process has three stages. The method shows high predictive accuracy, with low mean absolute and relative errors.",
          date: "2023-10-25",
          tags: "Other",
          img: "images/papers-images/RWKV-for-Gas-Sensors.png",
          link: "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4612708"
        },
        {
          title: "Exploring RWKV for Memory Efficient and Low Latency Streaming ASR",
          description: "The paper proposes applying RWKV, a linear attention transformer variant, to streaming ASR. It combines transformer performance and RNN inference efficiency. Experiments show RWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve good accuracy with minimal latency and memory cost.",
          date: "2023-09-26",
          tags: "Audio",
          img: "images/papers-images/RWKV-ASR-architecture.png",
          link: "https://arxiv.org/abs/2309.14758"
        },
        {
          title: "RRWKV: Capturing Long-range Dependencies in RWKV",
          description: "Transformers dominate NLP tasks. RWKV, a non-transformer architecture, aims to address dot-product attention drawbacks. But it has limitations in capturing long-range dependencies. Thus, the paper devises RRWKV by adding retrospecting ability to RWKV, maintaining efficiency.",
          date: "2023-06-09",
          tags: "Language Model",
          img: "images/papers-images/rrwkv-architecture.png",
          link: "https://arxiv.org/abs/2306.05176"
        },
        {
          title: "RWKV: Reinventing RNNs for the Transformer Era",
          description: "The paper proposes RWKV, a novel model architecture. It combines the efficient parallelizable training of transformers with the efficient inference of RNNs. RWKV uses a linear attention mechanism, scales to 14 billion parameters, and performs comparably to similar-sized transformers, advancing sequence processing tasks.",
          date: "2023-05-22",
          tags: "Language Model",
          img: "images/papers-images/rwkv-4.png",
          link: "https://arxiv.org/abs/2305.13048"
        }
      ];


      let currentProjects = allProjects; // 当前显示的项目列表
      let currentPage = 1;
      const itemsPerPage = 4; // 每页项目数量，确保为偶数以适配两列布局

      function filterProjects(category, element) {
        // 先移除所有按钮的高亮
        document.querySelectorAll(".filter-btn").forEach((btn) => {
          btn.classList.remove("bg-blue-500", "text-white");
          btn.classList.add("bg-gray-200", "text-gray-700");
        });

        // 给当前点击的按钮添加高亮
        element.classList.add("bg-blue-500", "text-white");
        element.classList.remove("bg-gray-200", "text-gray-700");

        // 过滤项目
        currentProjects =
          category === "全部"
            ? allProjects
            : allProjects.filter((project) => project.tags.includes(category));
        currentPage = 1; // 重置为第一页
        displayProjects();
        renderPagination();
        updateTotalCount();
      }

      function displayProjects() {
        const projectGrid = document.getElementById("projectGrid");
        projectGrid.innerHTML = ""; // 清空当前项目

        // 获取当前页项目
        const start = (currentPage - 1) * itemsPerPage;
        const end = start + itemsPerPage;
        const projectsToDisplay = currentProjects.slice(start, end);

        // 动态创建项目卡片
        projectsToDisplay.forEach((project) => {
          const projectCard = document.createElement("div");
          projectCard.className =
            "bg-white rounded-lg overflow-hidden shadow border border-gray-200";

          projectCard.innerHTML = `
      <div class="aspect-w-16 aspect-h-9">
        <img src="${
          project.img
        }" alt="Project Image" class="w-full h-full object-contain p-2">
      </div>
      <div class="p-4">
        <h3 class="text-lg font-semibold text-gray-800 mb-2 project-card-title" title="${
          project.title
        }">
          ${
            project.title.length > 20
              ? project.title.slice(0, 20) + "..."
              : project.title
          }
        </h3>
        <p class="text-gray-600 text-sm leading-snug mb-4 project-card-description">${
          project.description
        }</p>
        <div class="flex items-center justify-between">
          <div class="text-gray-500 text-xs">#${project.tags}</div>
          <div class="text-gray-500 text-xs">${project.date}</div>
        </div>
      </div>
    `;

          projectGrid.appendChild(projectCard);
        });
      }

      function renderPagination() {
        const pagination = document.getElementById("pagination");
        pagination.innerHTML = ""; // 清空分页按钮

        const totalPages = Math.ceil(currentProjects.length / itemsPerPage);

        if (totalPages <= 1) {
          pagination.style.display = "none";
          return;
        } else {
          pagination.style.display = "flex";
        }

        // 前一页按钮
        const prevButton = document.createElement("button");
        prevButton.className = `px-2 py-1 text-gray-600 ${
          currentPage === 1 ? "opacity-50 cursor-not-allowed" : ""
        }`;
        prevButton.innerHTML = "&lt;";
        prevButton.onclick = () => {
          if (currentPage > 1) goToPage(currentPage - 1);
        };
        pagination.appendChild(prevButton);

        // 页码按钮
        for (let i = 1; i <= totalPages; i++) {
          const pageButton = document.createElement("button");
          pageButton.className = `px-3 py-1 rounded border ${
            i === currentPage
              ? "bg-blue-500 text-white"
              : "bg-gray-200 text-gray-700"
          }`;
          pageButton.innerText = i;
          pageButton.onclick = () => goToPage(i);
          pagination.appendChild(pageButton);
        }

        // 下一页按钮
        const nextButton = document.createElement("button");
        nextButton.className = `px-2 py-1 text-gray-600 ${
          currentPage === totalPages ? "opacity-50 cursor-not-allowed" : ""
        }`;
        nextButton.innerHTML = "&gt;";
        nextButton.onclick = () => {
          if (currentPage < totalPages) goToPage(currentPage + 1);
        };
        pagination.appendChild(nextButton);
      }

      function goToPage(page) {
        currentPage = page;
        displayProjects();
        renderPagination();
      }

      function updateTotalCount() {
        console.log(currentProjects);

        const totalCount = document.getElementById("totalCount");
        totalCount.innerText = `共 ${currentProjects.length} 篇相关文章`;

        if (currentProjects.length <= 4) {
          totalCount.style.display = "none"; // 隐藏分页
        }
      }

      // 初始化显示所有项目
      filterProjects("全部", document.querySelector(".filter-btn"));
    </script>
  </body>
</html>
